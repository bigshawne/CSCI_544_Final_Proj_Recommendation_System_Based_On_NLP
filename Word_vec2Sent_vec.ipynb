{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lyoko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 198\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(word_embed_path, train_path, test_path, train_output_path, test_output_path):\n",
    "    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 256,\n",
    "                    'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
    "    model = InferSent(params_model)\n",
    "    \n",
    "    model.set_w2v_path(word_embed_path)\n",
    "    model.build_vocab_k_words(K=100000)\n",
    "    \n",
    "    with open(train_path) as f:\n",
    "        all_train = json.load(f)\n",
    "        train_r_ids = list(all_train.keys())\n",
    "        train_sents = [' '.join(sent) for sent in all_train.values()]\n",
    "        del all_train \n",
    "    with open(test_path) as f:\n",
    "        all_test = json.load(f)\n",
    "        test_r_ids = list(all_test.keys())\n",
    "        test_sents = [' '.join(sent) for sent in all_test.values()]\n",
    "        del all_test\n",
    "        \n",
    "    train_embeddings = model.encode(train_sents, bsize=128, tokenize=False, verbose=True)\n",
    "    test_embeddings = model.encode(test_sents, bsize=128, tokenize=False, verbose = True)\n",
    "    \n",
    "    train_output = {train_r_ids[i]: train_embeddings[i].tolist() for i in range(len(train_r_ids))}\n",
    "    test_output =  {test_r_ids[i]: test_embeddings[i].tolist() for i in range(len(test_r_ids))}\n",
    "    \n",
    "    with open(train_output_path, 'w') as f:\n",
    "        json.dump(train_output, f)\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2(word_embed_path, train_path, test_path, train_output_path, test_output_path):\n",
    "    params_model = {'bsize': 64, 'word_emb_dim': 200, 'enc_lstm_dim': 256,\n",
    "                    'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
    "    model = InferSent(params_model)\n",
    "    \n",
    "    model.set_w2v_path(word_embed_path)\n",
    "    model.build_vocab_k_words(K=100000)\n",
    "    \n",
    "    with open(train_path) as f:\n",
    "        all_train = json.load(f)\n",
    "        train_r_ids = list(all_train.keys())\n",
    "        train_sents = [' '.join(sent) for sent in all_train.values()]\n",
    "        del all_train \n",
    "    with open(test_path) as f:\n",
    "        all_test = json.load(f)\n",
    "        test_r_ids = list(all_test.keys())\n",
    "        test_sents = [' '.join(sent) for sent in all_test.values()]\n",
    "        del all_test\n",
    "        \n",
    "    train_embeddings = model.encode(train_sents, bsize=128, tokenize=False, verbose=True)\n",
    "    test_embeddings = model.encode(test_sents, bsize=128, tokenize=False, verbose = True)\n",
    "    \n",
    "    train_output = {train_r_ids[i]: train_embeddings[i].tolist() for i in range(len(train_r_ids))}\n",
    "    test_output =  {test_r_ids[i]: test_embeddings[i].tolist() for i in range(len(test_r_ids))}\n",
    "    \n",
    "    with open(train_output_path, 'w') as f:\n",
    "        json.dump(train_output, f)\n",
    "    with open(test_output_path, 'w') as f:\n",
    "        json.dump(test_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Nb words kept : 8184217/8384217 (97.6%)\n",
      "Speed : 300.1 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2044461/2094461 (97.6%)\n",
      "Speed : 253.6 sentences/s (cpu mode, bsize=128)\n",
      "Vocab size : 100000\n",
      "Nb words kept : 8184217/8384217 (97.6%)\n",
      "Speed : 235.1 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2044461/2094461 (97.6%)\n",
      "Speed : 239.8 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "# Glove\n",
    "m(\n",
    "    './Glove/input/glove_lemma.txt',\n",
    "    './revised_data/train/lemma_train.json',\n",
    "    './revised_data/test/lemma_test.json',\n",
    "    './Glove/glove_lemma_train.json',\n",
    "    './Glove/glove_lemma_test.json'\n",
    ")\n",
    "m(\n",
    "    './Glove/input/glove_no_lemma.txt',\n",
    "    './revised_data/train/no_lemma_train.json',\n",
    "    './revised_data/test/no_lemma_test.json',\n",
    "    './Glove/glove_no_lemma_train.json',\n",
    "    './Glove/glove_no_lemma_test.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Nb words kept : 8184217/8384217 (97.6%)\n",
      "Speed : 211.7 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2044461/2094461 (97.6%)\n",
      "Speed : 211.6 sentences/s (cpu mode, bsize=128)\n",
      "Vocab size : 100000\n",
      "Nb words kept : 8184217/8384217 (97.6%)\n",
      "Speed : 222.1 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2044461/2094461 (97.6%)\n",
      "Speed : 223.0 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "# Google\n",
    "m(\n",
    "    './Google/input/google_lemma.txt',\n",
    "    './revised_data/train/lemma_train.json',\n",
    "    './revised_data/test/lemma_test.json',\n",
    "    './Google/google_lemma_train.json',\n",
    "    './Google/google_lemma_test.json'\n",
    ")\n",
    "m(\n",
    "    './Google/input/google_no_lemma.txt',\n",
    "    './revised_data/train/no_lemma_train.json',\n",
    "    './revised_data/test/no_lemma_test.json',\n",
    "    './Google/google_no_lemma_train.json',\n",
    "    './Google/google_no_lemma_test.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Nb words kept : 8184217/8384217 (97.6%)\n",
      "Speed : 284.8 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2036794/2094461 (97.2%)\n",
      "Speed : 302.6 sentences/s (cpu mode, bsize=128)\n",
      "Vocab size : 100000\n",
      "Nb words kept : 8184217/8384217 (97.6%)\n",
      "Speed : 246.4 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2036732/2094461 (97.2%)\n",
      "Speed : 241.6 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "# Own model 300\n",
    "m(\n",
    "    './Own model/input/own_lemma.txt',\n",
    "    './revised_data/train/lemma_train.json',\n",
    "    './revised_data/test/lemma_test.json',\n",
    "    './Own model/own_lemma_train.json',\n",
    "    './Own model/own_lemma_test.json'\n",
    ")\n",
    "m(\n",
    "    './Own model/input/own_no_lemma.txt',\n",
    "    './revised_data/train/no_lemma_train.json',\n",
    "    './revised_data/test/no_lemma_test.json',\n",
    "    './Own model/own_no_lemma_train.json',\n",
    "    './Own model/own_no_lemma_test.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Nb words kept : 8184217/8384217 (97.6%)\n",
      "Speed : 372.3 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2036794/2094461 (97.2%)\n",
      "Speed : 389.6 sentences/s (cpu mode, bsize=128)\n",
      "Vocab size : 100000\n",
      "Nb words kept : 8184217/8384217 (97.6%)\n",
      "Speed : 364.5 sentences/s (cpu mode, bsize=128)\n",
      "Nb words kept : 2036732/2094461 (97.2%)\n",
      "Speed : 361.3 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "# Own model 300\n",
    "m2(\n",
    "    './Own model/input/own200_lemma.txt',\n",
    "    './revised_data/train/lemma_train.json',\n",
    "    './revised_data/test/lemma_test.json',\n",
    "    './Own model/own200_lemma_train.json',\n",
    "    './Own model/own200_lemma_test.json'\n",
    ")\n",
    "m2(\n",
    "    './Own model/input/own200_no_lemma.txt',\n",
    "    './revised_data/train/no_lemma_train.json',\n",
    "    './revised_data/test/no_lemma_test.json',\n",
    "    './Own model/own200_no_lemma_train.json',\n",
    "    './Own model/own200_no_lemma_test.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
